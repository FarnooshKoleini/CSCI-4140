{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIQ7ULuZ8dF9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2023</h6></center>\n",
    "<center><h6>Homework 1 - Tokenization and segmentation</h6></center>\n",
    "<center><h6>Due Sunday, January 29, at 11:59 PM</h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bdvfF8W8dGA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Tokenizer basics - 30 points\n",
    "<a id='part1'></a>\n",
    "## Part 1(a) - 10 points\n",
    "\n",
    "Write a function called `get_words` that takes a string `s` as its only argument. The function should return a list of the words in the same order as they appeared in `s`. Note that in this question a “word” is defined as a “space-separated item”. For example:\n",
    "```\n",
    "get_words('The cat in the hat ate the rat in the vat')\n",
    "\n",
    "['The', 'cat', 'in', 'the', 'hat', 'ate', 'the', 'rat', 'in', 'the', 'vat']\n",
    "```\n",
    "Hint: If you don’t know how to approach this problem, read about [str.split()](https://docs.python.org/3/library/stdtypes.html#str.split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'farnoosh', 'koleini']\n"
     ]
    }
   ],
   "source": [
    "# First writing a function from the scratch to practice what we want \n",
    "# for this question. And then after playing with that, we're gonna write a clean \n",
    "# get_words function easily. For this simple function, I get a sentence as an\n",
    "# input then we change that sentence the lopwercase and lastly we're gonna split it\n",
    "# and get the words.\n",
    "\n",
    "def get_words():\n",
    "    sentence = input(\"Write a sentence:\")\n",
    "    sentence = sentence.lower()\n",
    "    split_sentence = str.split(sentence)\n",
    "    print(split_sentence)\n",
    "\n",
    "get_words() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'in', 'the', 'hat', 'ate', 'the', 'rat', 'in', 'the', 'vat']\n"
     ]
    }
   ],
   "source": [
    "# get_words function\n",
    "\n",
    "def get_words(s, do_lower=False):\n",
    "    words = s.split() \n",
    "    if (do_lower): \n",
    "        s = s.lower() \n",
    "    return s.split() \n",
    "\n",
    "print (get_words('The cat in the hat ate the rat  in the vat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qpxKaA8mVzB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1(b) - 10 points\n",
    "\n",
    "Write a function called `count_words` that takes a list of the words of `s` as its only argument and returns a `collections.Counter` that maps a word to the frequency that it occurred in `s`. Use the output of the `get_words` function as the input to this function.\n",
    "\n",
    "```\n",
    "s = 'The cat in the hat ate the rat in the vat'\n",
    "words = get_words(s)\n",
    "count_words(words)\n",
    "\n",
    "Counter({'the': 3, 'in': 2, 'The': 1, 'cat': 1, 'hat': 1, 'ate': 1, 'rat': 1, 'vat': 1})\n",
    "```\n",
    "Notice that this is somewhat unsatisfying because **the** is counted separately from **The**. To fix this, have your `get_words` function be able to lower-case all of the words before returning them. You won’t want to break any previous code you wrote, though (backwards compatibility is important!), so add a new parameter to `get_words` with a default value:\n",
    "\n",
    "```\n",
    "def get_words(s, do_lower=False)\n",
    "```\n",
    "\n",
    "Now, if `get_words` is called the way we were using it above, nothing will change. But if we call `get_words(s, do_lower=True)` then `get_words` should lowercase the string before getting the words. You can make use of `str.lower` to modify the string. When you’re done, the following should work:\n",
    "\n",
    "```\n",
    "s = 'The cat in the hat ate the rat in the vat'\n",
    "words = get_words(s, do_lower=True)\n",
    "count_words(words)\n",
    "\n",
    "Counter({'the': 4, 'in': 2, 'cat': 1, 'hat': 1, 'ate': 1, 'rat': 1, 'vat': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'there': 1,\n",
       "         'are': 1,\n",
       "         'many': 1,\n",
       "         'students': 1,\n",
       "         'in': 2,\n",
       "         'our': 1,\n",
       "         'class': 1,\n",
       "         'who': 1,\n",
       "         'like': 1,\n",
       "         'get': 1,\n",
       "         'a': 1,\n",
       "         'great': 1,\n",
       "         'job': 1,\n",
       "         'the': 1,\n",
       "         'future': 1,\n",
       "         'soon': 1})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again for writing the count word function, first we need to write a simple \n",
    "# similiar function and get a general input like any sentences, then using the \n",
    "# previous small function, get_words, and then adding another part to that which\n",
    "# is counting the number of each word.\n",
    "\n",
    "import collections # importing the collection module.\n",
    "\n",
    "def count_words():\n",
    "    sentence = input(\"Write a sentence:\")\n",
    "    sentence = sentence.lower()\n",
    "    split_sentence = str.split(sentence)\n",
    "    return(collections.Counter(split_sentence))\n",
    "    \n",
    "count_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 4, 'in': 2, 'cat': 1, 'hat': 1, 'ate': 1, 'rat': 1, 'vat': 1})\n"
     ]
    }
   ],
   "source": [
    "# count_words function\n",
    "\n",
    "import collections\n",
    "\n",
    "def get_words(s, do_lower=False):\n",
    "    words = s.split() \n",
    "    if (do_lower): \n",
    "        s = s.lower() \n",
    "    return s.split() \n",
    "\n",
    "\n",
    "def count_words(words): \n",
    "   count_words = collections.Counter(words)\n",
    "   return (count_words)\n",
    "   \n",
    "\n",
    "\n",
    "s = 'The cat in the hat ate the rat in the vat'\n",
    "words = get_words(s, do_lower=True)\n",
    "print(count_words(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1(c) - 10 points\n",
    "\n",
    "Write a function called `words_by_frequency` that takes a list of words as its only required argument. The function should return a list of `(word, count)` tuples sorted by count such that the first item in the list is the most frequent item. Items with the same frequency should be in the same order they appear in the original list of words.\n",
    "\n",
    "`words_by_frequency` should, additionally, take a second parameter `n` that specifies the maximum number of results to return. If `n` is passed, then only the `n` most frequent words should be returned. If `n` is not passed, then all words should be returned in order of frequency.\n",
    "\n",
    "```\n",
    "words_by_frequency(words)\n",
    "\n",
    "[('the', 4), ('in', 2), ('cat', 1), ('hat', 1), ('ate', 1), ('rat', 1), ('vat', 1)]\n",
    "\n",
    "\n",
    "words_by_frequency(words, n=3)\n",
    "\n",
    "[('the', 4), ('in', 2), ('cat', 1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 2), ('there', 1), ('a', 1)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words_by_frequency function: for this section, again we need to write a simple \n",
    "# function which find the most common words with high frequency in a sentence or \n",
    "# a text file. For example, simply we can get a sentence as an input and then \n",
    "# finding the three most common word in that sentence. \n",
    "\n",
    "import collections\n",
    "\n",
    "def words_by_frequency():\n",
    "    sentence = input(\"Write a sentence:\")\n",
    "    n = int(input(\"Maximum number of results to return: \"))\n",
    "    sentence = sentence.lower()\n",
    "    split_sentence = str.split(sentence)\n",
    "    freq = collections.Counter(split_sentence)\n",
    "    return freq.most_common(n)\n",
    "\n",
    "words_by_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4), ('in', 2), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "# words_by_frequency function\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def words_by_frequency(word, n = None): \n",
    "    word_count = Counter(words) \n",
    "    return count_words(words).most_common(n) \n",
    "\n",
    "print (words_by_frequency(words, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Part 2: Through the rabbit hole - 50 points\n",
    "\n",
    "Next, you will explore some files from [Project Gutenberg](https://www.gutenberg.org), a library of free eBooks for texts outside of copyright.\n",
    "\n",
    "Some of the Gutenberg texts are all available in the `data/gutenberg/` directory.\n",
    "\n",
    "\n",
    "## Part 2(a) - 10 points\n",
    "<a id='part2a'></a>\n",
    "Let's use the copy of Lewis Carroll’s [“Alice’s Adventures in Wonderland”](https://www.gutenberg.org/ebooks/28885) from **data/gutenberg/carroll-alice.txt**. Use your `words_by_frequency` and `count_words` functions from [Part 1](#part1) to explore the text. For the rest of this exercise, you will always lowercase when getting a list of words. You should find that the five most frequent words in the text are:\n",
    "\n",
    "```\n",
    "the      1603\n",
    "and       766\n",
    "to        706\n",
    "a         614\n",
    "she       518\n",
    "```\n",
    "\n",
    "**Note:** If your numbers were right in the previous part, but don’t match here, it may be because of how you’re calling `split`. Take a look at the documentation for `split` to see if there’s a different way you can call it.\n",
    "\n",
    "**Check-In**\n",
    "\n",
    "1. If your `count_words` function is working correctly, it should report that the word **alice** occurs 221 times. Confirm that you get this result with your code.\n",
    "2. The word **alice** actually appears 398 times in the text, though this is not the answer you got for the previous question. Why? Examine the data to see if you can figure it out before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"[alice's\": 1,\n",
       "         'adventures': 4,\n",
       "         'in': 351,\n",
       "         'wonderland': 2,\n",
       "         'by': 57,\n",
       "         'lewis': 1,\n",
       "         'carroll': 1,\n",
       "         '1865]': 1,\n",
       "         'chapter': 12,\n",
       "         'i.': 1,\n",
       "         'down': 78,\n",
       "         'the': 1603,\n",
       "         'rabbit-hole': 3,\n",
       "         'alice': 221,\n",
       "         'was': 333,\n",
       "         'beginning': 11,\n",
       "         'to': 706,\n",
       "         'get': 43,\n",
       "         'very': 139,\n",
       "         'tired': 7,\n",
       "         'of': 493,\n",
       "         'sitting': 10,\n",
       "         'her': 208,\n",
       "         'sister': 5,\n",
       "         'on': 142,\n",
       "         'bank,': 2,\n",
       "         'and': 766,\n",
       "         'having': 10,\n",
       "         'nothing': 22,\n",
       "         'do:': 1,\n",
       "         'once': 19,\n",
       "         'or': 68,\n",
       "         'twice': 1,\n",
       "         'she': 518,\n",
       "         'had': 176,\n",
       "         'peeped': 3,\n",
       "         'into': 67,\n",
       "         'book': 3,\n",
       "         'reading,': 1,\n",
       "         'but': 116,\n",
       "         'it': 362,\n",
       "         'no': 67,\n",
       "         'pictures': 4,\n",
       "         'conversations': 1,\n",
       "         'it,': 38,\n",
       "         \"'and\": 55,\n",
       "         'what': 91,\n",
       "         'is': 71,\n",
       "         'use': 16,\n",
       "         'a': 614,\n",
       "         \"book,'\": 2,\n",
       "         'thought': 63,\n",
       "         \"'without\": 1,\n",
       "         \"conversation?'\": 1,\n",
       "         'so': 126,\n",
       "         'considering': 3,\n",
       "         'own': 9,\n",
       "         'mind': 5,\n",
       "         '(as': 3,\n",
       "         'well': 27,\n",
       "         'as': 249,\n",
       "         'could,': 7,\n",
       "         'for': 135,\n",
       "         'hot': 4,\n",
       "         'day': 11,\n",
       "         'made': 29,\n",
       "         'feel': 8,\n",
       "         'sleepy': 3,\n",
       "         'stupid),': 1,\n",
       "         'whether': 11,\n",
       "         'pleasure': 2,\n",
       "         'making': 8,\n",
       "         'daisy-chain': 1,\n",
       "         'would': 76,\n",
       "         'be': 138,\n",
       "         'worth': 4,\n",
       "         'trouble': 4,\n",
       "         'getting': 21,\n",
       "         'up': 81,\n",
       "         'picking': 2,\n",
       "         'daisies,': 1,\n",
       "         'when': 73,\n",
       "         'suddenly': 10,\n",
       "         'white': 28,\n",
       "         'rabbit': 30,\n",
       "         'with': 169,\n",
       "         'pink': 1,\n",
       "         'eyes': 18,\n",
       "         'ran': 13,\n",
       "         'close': 12,\n",
       "         'her.': 13,\n",
       "         'there': 64,\n",
       "         'remarkable': 2,\n",
       "         'that;': 1,\n",
       "         'nor': 2,\n",
       "         'did': 50,\n",
       "         'think': 38,\n",
       "         'much': 40,\n",
       "         'out': 97,\n",
       "         'way': 37,\n",
       "         'hear': 14,\n",
       "         'say': 35,\n",
       "         'itself,': 4,\n",
       "         \"'oh\": 2,\n",
       "         'dear!': 9,\n",
       "         'oh': 6,\n",
       "         'i': 260,\n",
       "         'shall': 22,\n",
       "         \"late!'\": 1,\n",
       "         '(when': 1,\n",
       "         'over': 31,\n",
       "         'afterwards,': 1,\n",
       "         'occurred': 2,\n",
       "         'that': 222,\n",
       "         'ought': 13,\n",
       "         'have': 77,\n",
       "         'wondered': 1,\n",
       "         'at': 205,\n",
       "         'this,': 17,\n",
       "         'time': 47,\n",
       "         'all': 155,\n",
       "         'seemed': 27,\n",
       "         'quite': 55,\n",
       "         'natural);': 1,\n",
       "         'actually': 1,\n",
       "         'took': 24,\n",
       "         'watch': 6,\n",
       "         'its': 57,\n",
       "         'waistcoat-pocket,': 2,\n",
       "         'looked': 45,\n",
       "         'then': 59,\n",
       "         'hurried': 11,\n",
       "         'on,': 27,\n",
       "         'started': 2,\n",
       "         'feet,': 6,\n",
       "         'flashed': 1,\n",
       "         'across': 5,\n",
       "         'never': 40,\n",
       "         'before': 19,\n",
       "         'seen': 12,\n",
       "         'either': 6,\n",
       "         'take': 19,\n",
       "         'burning': 1,\n",
       "         'curiosity,': 2,\n",
       "         'field': 1,\n",
       "         'after': 40,\n",
       "         'fortunately': 1,\n",
       "         'just': 48,\n",
       "         'see': 48,\n",
       "         'pop': 1,\n",
       "         'large': 32,\n",
       "         'under': 16,\n",
       "         'hedge.': 1,\n",
       "         'another': 21,\n",
       "         'moment': 21,\n",
       "         'went': 79,\n",
       "         'how': 46,\n",
       "         'world': 6,\n",
       "         'again.': 16,\n",
       "         'straight': 2,\n",
       "         'like': 75,\n",
       "         'tunnel': 1,\n",
       "         'some': 50,\n",
       "         'way,': 7,\n",
       "         'dipped': 2,\n",
       "         'down,': 14,\n",
       "         'not': 108,\n",
       "         'about': 84,\n",
       "         'stopping': 1,\n",
       "         'herself': 40,\n",
       "         'found': 28,\n",
       "         'falling': 2,\n",
       "         'deep': 5,\n",
       "         'well.': 2,\n",
       "         'deep,': 2,\n",
       "         'fell': 6,\n",
       "         'slowly,': 2,\n",
       "         'plenty': 2,\n",
       "         'look': 25,\n",
       "         'wonder': 15,\n",
       "         'going': 26,\n",
       "         'happen': 5,\n",
       "         'next.': 3,\n",
       "         'first,': 11,\n",
       "         'tried': 18,\n",
       "         'make': 26,\n",
       "         'coming': 5,\n",
       "         'to,': 3,\n",
       "         'too': 21,\n",
       "         'dark': 3,\n",
       "         'anything;': 2,\n",
       "         'sides': 4,\n",
       "         'well,': 1,\n",
       "         'noticed': 7,\n",
       "         'they': 117,\n",
       "         'were': 82,\n",
       "         'filled': 3,\n",
       "         'cupboards': 2,\n",
       "         'book-shelves;': 1,\n",
       "         'here': 19,\n",
       "         'saw': 13,\n",
       "         'maps': 1,\n",
       "         'hung': 1,\n",
       "         'upon': 26,\n",
       "         'pegs.': 1,\n",
       "         'jar': 2,\n",
       "         'from': 32,\n",
       "         'one': 80,\n",
       "         'shelves': 1,\n",
       "         'passed;': 1,\n",
       "         'labelled': 1,\n",
       "         \"'orange\": 1,\n",
       "         \"marmalade',\": 1,\n",
       "         'great': 39,\n",
       "         'disappointment': 1,\n",
       "         'empty:': 1,\n",
       "         'drop': 1,\n",
       "         'fear': 4,\n",
       "         'killing': 1,\n",
       "         'somebody,': 1,\n",
       "         'managed': 3,\n",
       "         'put': 31,\n",
       "         'past': 1,\n",
       "         'it.': 16,\n",
       "         \"'well!'\": 1,\n",
       "         'herself,': 31,\n",
       "         \"'after\": 2,\n",
       "         'such': 40,\n",
       "         'fall': 6,\n",
       "         'tumbling': 2,\n",
       "         'stairs!': 1,\n",
       "         'brave': 1,\n",
       "         \"they'll\": 4,\n",
       "         'me': 46,\n",
       "         'home!': 1,\n",
       "         'why,': 9,\n",
       "         \"wouldn't\": 12,\n",
       "         'anything': 14,\n",
       "         'even': 19,\n",
       "         'if': 72,\n",
       "         'off': 40,\n",
       "         'top': 8,\n",
       "         \"house!'\": 1,\n",
       "         '(which': 3,\n",
       "         'likely': 4,\n",
       "         'true.)': 1,\n",
       "         'down.': 2,\n",
       "         'come': 26,\n",
       "         'an': 56,\n",
       "         'end!': 1,\n",
       "         \"'i\": 121,\n",
       "         'many': 12,\n",
       "         'miles': 3,\n",
       "         \"i've\": 20,\n",
       "         'fallen': 4,\n",
       "         'this': 103,\n",
       "         \"time?'\": 1,\n",
       "         'said': 421,\n",
       "         'aloud.': 3,\n",
       "         'must': 42,\n",
       "         'somewhere': 1,\n",
       "         'near': 14,\n",
       "         'centre': 1,\n",
       "         'earth.': 2,\n",
       "         'let': 13,\n",
       "         'see:': 3,\n",
       "         'four': 6,\n",
       "         'thousand': 2,\n",
       "         \"think--'\": 3,\n",
       "         '(for,': 2,\n",
       "         'you': 264,\n",
       "         'see,': 11,\n",
       "         'learnt': 2,\n",
       "         'several': 4,\n",
       "         'things': 19,\n",
       "         'sort': 17,\n",
       "         'lessons': 4,\n",
       "         'schoolroom,': 1,\n",
       "         'though': 7,\n",
       "         'good': 23,\n",
       "         'opportunity': 8,\n",
       "         'showing': 2,\n",
       "         'knowledge,': 1,\n",
       "         'listen': 4,\n",
       "         'her,': 18,\n",
       "         'still': 13,\n",
       "         'practice': 1,\n",
       "         'over)': 1,\n",
       "         \"'--yes,\": 1,\n",
       "         \"that's\": 17,\n",
       "         'right': 21,\n",
       "         'distance--but': 1,\n",
       "         'latitude': 2,\n",
       "         'longitude': 2,\n",
       "         'got': 45,\n",
       "         \"to?'\": 3,\n",
       "         '(alice': 4,\n",
       "         'idea': 14,\n",
       "         'was,': 14,\n",
       "         'either,': 2,\n",
       "         'nice': 5,\n",
       "         'grand': 2,\n",
       "         'words': 14,\n",
       "         'say.)': 1,\n",
       "         'presently': 2,\n",
       "         'began': 47,\n",
       "         'through': 13,\n",
       "         'earth!': 1,\n",
       "         'funny': 3,\n",
       "         \"it'll\": 7,\n",
       "         'seem': 7,\n",
       "         'among': 12,\n",
       "         'people': 10,\n",
       "         'walk': 4,\n",
       "         'their': 51,\n",
       "         'heads': 8,\n",
       "         'downward!': 1,\n",
       "         'antipathies,': 1,\n",
       "         '(she': 9,\n",
       "         'rather': 25,\n",
       "         'glad': 11,\n",
       "         'listening,': 2,\n",
       "         'time,': 7,\n",
       "         \"didn't\": 11,\n",
       "         'sound': 3,\n",
       "         'word)': 1,\n",
       "         \"'--but\": 1,\n",
       "         'ask': 7,\n",
       "         'them': 49,\n",
       "         'name': 8,\n",
       "         'country': 1,\n",
       "         'is,': 16,\n",
       "         'know.': 8,\n",
       "         'please,': 4,\n",
       "         \"ma'am,\": 1,\n",
       "         'new': 5,\n",
       "         'zealand': 1,\n",
       "         \"australia?'\": 1,\n",
       "         '(and': 1,\n",
       "         'curtsey': 1,\n",
       "         'spoke--fancy': 1,\n",
       "         'curtseying': 1,\n",
       "         \"you're\": 17,\n",
       "         'air!': 1,\n",
       "         'do': 51,\n",
       "         'could': 65,\n",
       "         'manage': 6,\n",
       "         'it?)': 1,\n",
       "         'ignorant': 1,\n",
       "         'little': 120,\n",
       "         'girl': 3,\n",
       "         \"she'll\": 3,\n",
       "         'asking!': 1,\n",
       "         'no,': 4,\n",
       "         'ask:': 1,\n",
       "         'perhaps': 12,\n",
       "         'written': 6,\n",
       "         \"somewhere.'\": 1,\n",
       "         'else': 8,\n",
       "         'do,': 7,\n",
       "         'soon': 24,\n",
       "         'talking': 12,\n",
       "         \"'dinah'll\": 1,\n",
       "         'miss': 1,\n",
       "         'to-night,': 1,\n",
       "         'should': 27,\n",
       "         \"think!'\": 1,\n",
       "         '(dinah': 1,\n",
       "         'cat.)': 1,\n",
       "         'hope': 3,\n",
       "         'remember': 12,\n",
       "         'saucer': 1,\n",
       "         'milk': 1,\n",
       "         'tea-time.': 1,\n",
       "         'dinah': 4,\n",
       "         'my': 56,\n",
       "         'wish': 21,\n",
       "         'me!': 3,\n",
       "         'are': 40,\n",
       "         'mice': 3,\n",
       "         'air,': 5,\n",
       "         \"i'm\": 37,\n",
       "         'afraid,': 2,\n",
       "         'might': 27,\n",
       "         'catch': 3,\n",
       "         'bat,': 1,\n",
       "         'mouse,': 11,\n",
       "         'cats': 9,\n",
       "         'eat': 16,\n",
       "         'bats,': 1,\n",
       "         \"wonder?'\": 3,\n",
       "         'sleepy,': 1,\n",
       "         'saying': 11,\n",
       "         'dreamy': 1,\n",
       "         \"'do\": 8,\n",
       "         'bats?': 1,\n",
       "         \"bats?'\": 1,\n",
       "         'sometimes,': 1,\n",
       "         'bats': 1,\n",
       "         \"cats?'\": 1,\n",
       "         'for,': 3,\n",
       "         \"couldn't\": 9,\n",
       "         'answer': 6,\n",
       "         'question,': 4,\n",
       "         'matter': 8,\n",
       "         'which': 40,\n",
       "         'felt': 23,\n",
       "         'dozing': 1,\n",
       "         'off,': 14,\n",
       "         'begun': 6,\n",
       "         'dream': 3,\n",
       "         'walking': 5,\n",
       "         'hand': 11,\n",
       "         'dinah,': 3,\n",
       "         'earnestly,': 1,\n",
       "         \"'now,\": 4,\n",
       "         'tell': 29,\n",
       "         'truth:': 1,\n",
       "         'ever': 17,\n",
       "         \"bat?'\": 1,\n",
       "         'suddenly,': 1,\n",
       "         'thump!': 2,\n",
       "         'came': 38,\n",
       "         'heap': 1,\n",
       "         'sticks': 1,\n",
       "         'dry': 7,\n",
       "         'leaves,': 2,\n",
       "         'over.': 2,\n",
       "         'bit': 8,\n",
       "         'hurt,': 1,\n",
       "         'jumped': 5,\n",
       "         'feet': 11,\n",
       "         'moment:': 1,\n",
       "         'up,': 9,\n",
       "         'overhead;': 1,\n",
       "         'long': 30,\n",
       "         'passage,': 2,\n",
       "         'sight,': 4,\n",
       "         'hurrying': 1,\n",
       "         'lost:': 1,\n",
       "         'away': 15,\n",
       "         'wind,': 2,\n",
       "         'say,': 5,\n",
       "         'turned': 16,\n",
       "         'corner,': 2,\n",
       "         'ears': 4,\n",
       "         'whiskers,': 1,\n",
       "         'late': 3,\n",
       "         \"it's\": 31,\n",
       "         \"getting!'\": 1,\n",
       "         'behind': 12,\n",
       "         'longer': 2,\n",
       "         'seen:': 1,\n",
       "         'long,': 1,\n",
       "         'low': 7,\n",
       "         'hall,': 5,\n",
       "         'lit': 1,\n",
       "         'row': 2,\n",
       "         'lamps': 1,\n",
       "         'hanging': 3,\n",
       "         'roof.': 1,\n",
       "         'doors': 2,\n",
       "         'round': 30,\n",
       "         'locked;': 1,\n",
       "         'been': 36,\n",
       "         'side': 12,\n",
       "         'other,': 5,\n",
       "         'trying': 11,\n",
       "         'every': 12,\n",
       "         'door,': 9,\n",
       "         'walked': 10,\n",
       "         'sadly': 2,\n",
       "         'middle,': 3,\n",
       "         'wondering': 7,\n",
       "         'three-legged': 2,\n",
       "         'table,': 5,\n",
       "         'solid': 1,\n",
       "         'glass;': 1,\n",
       "         'except': 4,\n",
       "         'tiny': 4,\n",
       "         'golden': 7,\n",
       "         'key,': 3,\n",
       "         \"alice's\": 10,\n",
       "         'first': 30,\n",
       "         'belong': 1,\n",
       "         'hall;': 1,\n",
       "         'but,': 8,\n",
       "         'alas!': 3,\n",
       "         'locks': 1,\n",
       "         'large,': 1,\n",
       "         'key': 5,\n",
       "         'small,': 1,\n",
       "         'any': 36,\n",
       "         'rate': 4,\n",
       "         'open': 6,\n",
       "         'them.': 2,\n",
       "         'however,': 19,\n",
       "         'second': 4,\n",
       "         'round,': 7,\n",
       "         'curtain': 1,\n",
       "         'before,': 11,\n",
       "         'door': 15,\n",
       "         'fifteen': 1,\n",
       "         'inches': 6,\n",
       "         'high:': 3,\n",
       "         'lock,': 1,\n",
       "         'delight': 1,\n",
       "         'fitted!': 1,\n",
       "         'opened': 9,\n",
       "         'led': 4,\n",
       "         'small': 8,\n",
       "         'larger': 3,\n",
       "         'than': 23,\n",
       "         'rat-hole:': 1,\n",
       "         'knelt': 1,\n",
       "         'along': 5,\n",
       "         'passage': 1,\n",
       "         'loveliest': 1,\n",
       "         'garden': 4,\n",
       "         'saw.': 1,\n",
       "         'longed': 2,\n",
       "         'wander': 1,\n",
       "         'those': 10,\n",
       "         'beds': 1,\n",
       "         'bright': 7,\n",
       "         'flowers': 2,\n",
       "         'cool': 2,\n",
       "         'fountains,': 1,\n",
       "         'head': 29,\n",
       "         'doorway;': 1,\n",
       "         'go': 39,\n",
       "         \"through,'\": 1,\n",
       "         'poor': 26,\n",
       "         'alice,': 76,\n",
       "         \"'it\": 30,\n",
       "         'without': 24,\n",
       "         'shoulders.': 1,\n",
       "         'oh,': 6,\n",
       "         'shut': 4,\n",
       "         'telescope!': 1,\n",
       "         'only': 44,\n",
       "         'know': 46,\n",
       "         \"begin.'\": 3,\n",
       "         'out-of-the-way': 3,\n",
       "         'happened': 3,\n",
       "         'lately,': 1,\n",
       "         'few': 9,\n",
       "         'indeed': 3,\n",
       "         'really': 9,\n",
       "         'impossible.': 1,\n",
       "         'waiting': 8,\n",
       "         'back': 29,\n",
       "         'half': 21,\n",
       "         'hoping': 3,\n",
       "         'find': 20,\n",
       "         'rules': 3,\n",
       "         'shutting': 2,\n",
       "         'telescopes:': 1,\n",
       "         'bottle': 7,\n",
       "         \"('which\": 1,\n",
       "         'certainly': 8,\n",
       "         \"before,'\": 3,\n",
       "         'alice,)': 2,\n",
       "         'neck': 6,\n",
       "         'paper': 3,\n",
       "         'label,': 1,\n",
       "         \"'drink\": 3,\n",
       "         \"me'\": 2,\n",
       "         'beautifully': 2,\n",
       "         'printed': 1,\n",
       "         'letters.': 1,\n",
       "         \"me,'\": 8,\n",
       "         'wise': 2,\n",
       "         'hurry.': 3,\n",
       "         \"'no,\": 9,\n",
       "         \"i'll\": 24,\n",
       "         \"first,'\": 2,\n",
       "         'said,': 26,\n",
       "         'marked': 5,\n",
       "         '\"poison\"': 1,\n",
       "         \"not';\": 1,\n",
       "         'read': 9,\n",
       "         'histories': 1,\n",
       "         'children': 5,\n",
       "         'who': 54,\n",
       "         'burnt,': 1,\n",
       "         'eaten': 1,\n",
       "         'wild': 2,\n",
       "         'beasts': 1,\n",
       "         'other': 26,\n",
       "         'unpleasant': 2,\n",
       "         'things,': 2,\n",
       "         'because': 12,\n",
       "         'simple': 5,\n",
       "         'friends': 2,\n",
       "         'taught': 4,\n",
       "         'them:': 1,\n",
       "         'as,': 2,\n",
       "         'red-hot': 1,\n",
       "         'poker': 1,\n",
       "         'will': 30,\n",
       "         'burn': 2,\n",
       "         'hold': 6,\n",
       "         'long;': 1,\n",
       "         'cut': 5,\n",
       "         'your': 60,\n",
       "         'finger': 3,\n",
       "         'deeply': 1,\n",
       "         'knife,': 1,\n",
       "         'usually': 2,\n",
       "         'bleeds;': 1,\n",
       "         'forgotten': 6,\n",
       "         'that,': 5,\n",
       "         'drink': 4,\n",
       "         \"'poison,'\": 2,\n",
       "         'almost': 6,\n",
       "         'certain': 2,\n",
       "         'disagree': 1,\n",
       "         'you,': 26,\n",
       "         'sooner': 2,\n",
       "         'later.': 1,\n",
       "         'ventured': 4,\n",
       "         'taste': 2,\n",
       "         'finding': 3,\n",
       "         'nice,': 1,\n",
       "         '(it': 5,\n",
       "         'had,': 1,\n",
       "         'fact,': 4,\n",
       "         'mixed': 2,\n",
       "         'flavour': 1,\n",
       "         'cherry-tart,': 1,\n",
       "         'custard,': 1,\n",
       "         'pine-apple,': 1,\n",
       "         'roast': 1,\n",
       "         'turkey,': 1,\n",
       "         'toffee,': 1,\n",
       "         'buttered': 1,\n",
       "         'toast,)': 1,\n",
       "         'finished': 7,\n",
       "         'off.': 5,\n",
       "         '*': 60,\n",
       "         \"'what\": 34,\n",
       "         'curious': 16,\n",
       "         \"feeling!'\": 1,\n",
       "         'alice;': 16,\n",
       "         \"telescope.'\": 1,\n",
       "         'indeed:': 1,\n",
       "         'now': 25,\n",
       "         'ten': 5,\n",
       "         'high,': 3,\n",
       "         'face': 7,\n",
       "         'brightened': 2,\n",
       "         'size': 6,\n",
       "         'lovely': 2,\n",
       "         'garden.': 3,\n",
       "         'waited': 9,\n",
       "         'minutes': 7,\n",
       "         'shrink': 1,\n",
       "         'further:': 1,\n",
       "         'nervous': 4,\n",
       "         'this;': 2,\n",
       "         \"'for\": 7,\n",
       "         'end,': 1,\n",
       "         \"know,'\": 8,\n",
       "         \"'in\": 8,\n",
       "         'altogether,': 2,\n",
       "         'candle.': 1,\n",
       "         \"then?'\": 1,\n",
       "         'fancy': 3,\n",
       "         'flame': 1,\n",
       "         'candle': 2,\n",
       "         'blown': 1,\n",
       "         'out,': 13,\n",
       "         'thing.': 1,\n",
       "         'while,': 4,\n",
       "         'more': 39,\n",
       "         'happened,': 2,\n",
       "         'decided': 3,\n",
       "         'once;': 1,\n",
       "         'alas': 1,\n",
       "         'alice!': 3,\n",
       "         'table': 8,\n",
       "         'possibly': 3,\n",
       "         'reach': 4,\n",
       "         'it:': 9,\n",
       "         'plainly': 1,\n",
       "         'glass,': 2,\n",
       "         'best': 9,\n",
       "         'climb': 1,\n",
       "         'legs': 3,\n",
       "         'slippery;': 1,\n",
       "         'trying,': 1,\n",
       "         'thing': 35,\n",
       "         'sat': 17,\n",
       "         'cried.': 2,\n",
       "         \"'come,\": 9,\n",
       "         \"there's\": 16,\n",
       "         'crying': 2,\n",
       "         \"that!'\": 8,\n",
       "         'sharply;': 1,\n",
       "         'advise': 1,\n",
       "         'leave': 7,\n",
       "         \"minute!'\": 1,\n",
       "         'generally': 5,\n",
       "         'gave': 15,\n",
       "         'advice,': 1,\n",
       "         '(though': 1,\n",
       "         'seldom': 1,\n",
       "         'followed': 8,\n",
       "         'it),': 2,\n",
       "         'sometimes': 4,\n",
       "         'scolded': 1,\n",
       "         'severely': 3,\n",
       "         'bring': 2,\n",
       "         'tears': 5,\n",
       "         'eyes;': 1,\n",
       "         'remembered': 5,\n",
       "         'box': 3,\n",
       "         'cheated': 1,\n",
       "         'game': 6,\n",
       "         'croquet': 3,\n",
       "         'playing': 2,\n",
       "         'against': 9,\n",
       "         'child': 3,\n",
       "         'fond': 3,\n",
       "         'pretending': 1,\n",
       "         'two': 22,\n",
       "         'people.': 1,\n",
       "         \"'but\": 38,\n",
       "         \"now,'\": 4,\n",
       "         \"'to\": 5,\n",
       "         'pretend': 1,\n",
       "         'people!': 1,\n",
       "         'hardly': 12,\n",
       "         'enough': 10,\n",
       "         'left': 13,\n",
       "         'respectable': 1,\n",
       "         \"person!'\": 1,\n",
       "         'eye': 4,\n",
       "         'glass': 4,\n",
       "         'lying': 8,\n",
       "         'table:': 1,\n",
       "         'cake,': 2,\n",
       "         \"'eat\": 1,\n",
       "         'currants.': 1,\n",
       "         \"'well,\": 20,\n",
       "         \"it,'\": 19,\n",
       "         'makes': 11,\n",
       "         'grow': 13,\n",
       "         'larger,': 3,\n",
       "         'can': 31,\n",
       "         'key;': 1,\n",
       "         'smaller,': 3,\n",
       "         'creep': 1,\n",
       "         'door;': 1,\n",
       "         'garden,': 5,\n",
       "         \"don't\": 53,\n",
       "         'care': 4,\n",
       "         \"happens!'\": 1,\n",
       "         'ate': 1,\n",
       "         'bit,': 2,\n",
       "         'anxiously': 13,\n",
       "         \"'which\": 3,\n",
       "         'way?': 1,\n",
       "         \"way?',\": 1,\n",
       "         'holding': 2,\n",
       "         'growing,': 4,\n",
       "         'surprised': 6,\n",
       "         'remained': 3,\n",
       "         'same': 21,\n",
       "         'size:': 3,\n",
       "         'sure,': 2,\n",
       "         'happens': 2,\n",
       "         'eats': 1,\n",
       "         'expecting': 3,\n",
       "         'happen,': 1,\n",
       "         'dull': 2,\n",
       "         'stupid': 1,\n",
       "         'life': 2,\n",
       "         'common': 1,\n",
       "         'way.': 3,\n",
       "         'set': 14,\n",
       "         'work,': 1,\n",
       "         'cake.': 1,\n",
       "         'ii.': 1,\n",
       "         'pool': 7,\n",
       "         \"'curiouser\": 1,\n",
       "         \"curiouser!'\": 1,\n",
       "         'cried': 18,\n",
       "         'surprised,': 1,\n",
       "         'forgot': 2,\n",
       "         'speak': 8,\n",
       "         'english);': 1,\n",
       "         \"'now\": 5,\n",
       "         'opening': 3,\n",
       "         'largest': 1,\n",
       "         'telescope': 1,\n",
       "         'was!': 1,\n",
       "         'good-bye,': 1,\n",
       "         \"feet!'\": 1,\n",
       "         '(for': 1,\n",
       "         'far': 9,\n",
       "         'off).': 1,\n",
       "         \"'oh,\": 19,\n",
       "         'shoes': 5,\n",
       "         'stockings': 1,\n",
       "         'now,': 7,\n",
       "         'dears?': 1,\n",
       "         'sure': 16,\n",
       "         '_i_': 2,\n",
       "         \"shan't\": 4,\n",
       "         'able!': 1,\n",
       "         'deal': 11,\n",
       "         'myself': 2,\n",
       "         'you:': 1,\n",
       "         'can;--but': 1,\n",
       "         'kind': 6,\n",
       "         \"them,'\": 3,\n",
       "         \"'or\": 6,\n",
       "         \"won't\": 21,\n",
       "         'want': 9,\n",
       "         'go!': 1,\n",
       "         'give': 9,\n",
       "         'pair': 5,\n",
       "         'boots': 3,\n",
       "         \"christmas.'\": 1,\n",
       "         'planning': 1,\n",
       "         \"'they\": 9,\n",
       "         \"carrier,'\": 1,\n",
       "         'thought;': 1,\n",
       "         'seem,': 1,\n",
       "         'sending': 2,\n",
       "         'presents': 2,\n",
       "         \"one's\": 1,\n",
       "         'feet!': 1,\n",
       "         'odd': 1,\n",
       "         'directions': 1,\n",
       "         'look!': 1,\n",
       "         'foot,': 3,\n",
       "         'esq.': 1,\n",
       "         'hearthrug,': 1,\n",
       "         'fender,': 1,\n",
       "         '(with': 2,\n",
       "         'love).': 1,\n",
       "         'dear,': 6,\n",
       "         'nonsense': 1,\n",
       "         \"talking!'\": 1,\n",
       "         'struck': 2,\n",
       "         'roof': 5,\n",
       "         'hall:': 1,\n",
       "         'fact': 2,\n",
       "         'nine': 4,\n",
       "         'door.': 2,\n",
       "         'side,': 3,\n",
       "         'eye;': 2,\n",
       "         'hopeless': 1,\n",
       "         'ever:': 1,\n",
       "         'cry': 3,\n",
       "         \"'you\": 39,\n",
       "         'ashamed': 2,\n",
       "         \"yourself,'\": 1,\n",
       "         \"'a\": 11,\n",
       "         \"you,'\": 6,\n",
       "         'this),': 1,\n",
       "         'way!': 1,\n",
       "         'stop': 4,\n",
       "         'moment,': 5,\n",
       "         \"you!'\": 3,\n",
       "         'same,': 2,\n",
       "         'shedding': 1,\n",
       "         'gallons': 1,\n",
       "         'tears,': 3,\n",
       "         'until': 4,\n",
       "         'reaching': 1,\n",
       "         'hall.': 1,\n",
       "         'heard': 29,\n",
       "         'pattering': 3,\n",
       "         'distance,': 4,\n",
       "         'hastily': 7,\n",
       "         'dried': 1,\n",
       "         'coming.': 2,\n",
       "         'returning,': 1,\n",
       "         'splendidly': 1,\n",
       "         'dressed,': 1,\n",
       "         'kid': 5,\n",
       "         'gloves': 5,\n",
       "         'fan': 8,\n",
       "         'other:': 3,\n",
       "         'he': 111,\n",
       "         'trotting': 2,\n",
       "         'hurry,': 1,\n",
       "         'muttering': 3,\n",
       "         'himself': 4,\n",
       "         'came,': 2,\n",
       "         \"'oh!\": 2,\n",
       "         'duchess,': 8,\n",
       "         'duchess!': 3,\n",
       "         'oh!': 3,\n",
       "         'savage': 3,\n",
       "         'kept': 13,\n",
       "         \"waiting!'\": 1,\n",
       "         'desperate': 1,\n",
       "         'ready': 7,\n",
       "         'help': 9,\n",
       "         'one;': 2,\n",
       "         'so,': 7,\n",
       "         'began,': 6,\n",
       "         'low,': 6,\n",
       "         'timid': 3,\n",
       "         'voice,': 15,\n",
       "         \"'if\": 21,\n",
       "         \"sir--'\": 1,\n",
       "         'violently,': 2,\n",
       "         'dropped': 4,\n",
       "         'fan,': 1,\n",
       "         'skurried': 1,\n",
       "         'darkness': 1,\n",
       "         'hard': 8,\n",
       "         'go.': 1,\n",
       "         'gloves,': 3,\n",
       "         'and,': 19,\n",
       "         'hall': 1,\n",
       "         'hot,': 1,\n",
       "         'fanning': 1,\n",
       "         'talking:': 1,\n",
       "         \"'dear,\": 1,\n",
       "         'queer': 9,\n",
       "         'everything': 10,\n",
       "         'to-day!': 1,\n",
       "         'yesterday': 2,\n",
       "         'usual.': 2,\n",
       "         'changed': 7,\n",
       "         'night?': 1,\n",
       "         'think:': 1,\n",
       "         'morning?': 1,\n",
       "         'feeling': 6,\n",
       "         'different.': 1,\n",
       "         'next': 22,\n",
       "         'question': 7,\n",
       "         'am': 13,\n",
       "         'i?': 1,\n",
       "         'ah,': 1,\n",
       "         \"puzzle!'\": 1,\n",
       "         'thinking': 10,\n",
       "         'knew': 12,\n",
       "         'age': 2,\n",
       "         \"'i'm\": 20,\n",
       "         \"ada,'\": 1,\n",
       "         'hair': 5,\n",
       "         'goes': 7,\n",
       "         'ringlets,': 1,\n",
       "         'mine': 3,\n",
       "         \"doesn't\": 16,\n",
       "         'ringlets': 1,\n",
       "         'all;': 2,\n",
       "         \"can't\": 27,\n",
       "         'mabel,': 2,\n",
       "         'sorts': 3,\n",
       "         'she,': 5,\n",
       "         'knows': 2,\n",
       "         'little!': 1,\n",
       "         'besides,': 2,\n",
       "         \"she's\": 4,\n",
       "         'i,': 1,\n",
       "         'and--oh': 2,\n",
       "         'puzzling': 4,\n",
       "         'is!': 1,\n",
       "         'try': 12,\n",
       "         'used': 12,\n",
       "         'times': 6,\n",
       "         'five': 2,\n",
       "         'twelve,': 1,\n",
       "         'six': 2,\n",
       "         'thirteen,': 1,\n",
       "         'seven': 4,\n",
       "         'is--oh': 1,\n",
       "         'twenty': 1,\n",
       "         'rate!': 1,\n",
       "         'multiplication': 1,\n",
       "         'signify:': 1,\n",
       "         \"let's\": 3,\n",
       "         'geography.': 1,\n",
       "         'london': 1,\n",
       "         'capital': 4,\n",
       "         'paris,': 1,\n",
       "         'paris': 1,\n",
       "         'rome,': 1,\n",
       "         'rome--no,': 1,\n",
       "         'wrong,': 2,\n",
       "         'certain!': 1,\n",
       "         'mabel!': 1,\n",
       "         '\"how': 2,\n",
       "         'doth': 3,\n",
       "         'little--\"\\'': 1,\n",
       "         'crossed': 3,\n",
       "         'hands': 6,\n",
       "         'lap': 2,\n",
       "         'lessons,': 1,\n",
       "         'repeat': 6,\n",
       "         'voice': 18,\n",
       "         ...})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function simply shows that the word alice occurs 221 times in the text file,\n",
    "# but actually the exact number is 398. This differnce is becauser sometimes there \n",
    "# some punctuations next to the word 'alice' so the machine recognize it as a different \n",
    "# alice. \n",
    "\n",
    "f = open('/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt', 'r')\n",
    "data = f.read()\n",
    "\n",
    "def count_words():\n",
    "    data1 = data.lower()\n",
    "    split_data = str.split(data1)\n",
    "    return(collections.Counter(split_data))\n",
    "    \n",
    "count_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1603), ('and', 766), ('to', 706), ('a', 614), ('she', 518)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can easily use both previous codes for words_by_frequency functions either the \n",
    "# scratch one or the final clean function to find five most frequent words \n",
    "# in the text. Now I would like to use that simple test code that I have developed \n",
    "# and then start implementing the final version words_by_frequency code to get the \n",
    "# result faster. Next part is creating a clean function and testing it again to see\n",
    "# we're going to get the same result with the first scratch and simple function.\n",
    "\n",
    "f = open('/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt', 'r')\n",
    "data = f.read()\n",
    "\n",
    "def words_by_frequency():\n",
    "    n = int(input(\"Maximum number of results to return: \"))\n",
    "    data1 = data.lower()\n",
    "    split_data = str.split(data1)\n",
    "    freq = collections.Counter(split_data)\n",
    "    return freq.most_common(n)\n",
    "\n",
    "words_by_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'alice' appears 221 time in the text.\n",
      "[('the', 1603), ('and', 766), ('to', 706), ('a', 614), ('she', 518)]\n"
     ]
    }
   ],
   "source": [
    "# clean version of the code and results\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def get_words(s, do_lower=False):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)\n",
    "    if(do_lower):\n",
    "        s = s.lower()\n",
    "        return s.split()\n",
    "    \n",
    "file_path = (r\"/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    file_contents = f.read()\n",
    "    words = get_words(file_contents, do_lower = True)\n",
    "    word_counts = count_words(words)\n",
    "    alice_count = word_counts.get(\"alice\",0)\n",
    "    word_frequency = words_by_frequency(words, n=5)\n",
    "    print(\"The word 'alice' appears\", alice_count, \"time in the text.\")\n",
    "    print(word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'alice' occurs = 398 times\n"
     ]
    }
   ],
   "source": [
    "#The word alice actually appears 398 times in the text, though this is not the answer you got for the previous question. Why? Examine the data to see if you can figure it out before continuing.\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def get_words(s, do_lower=False):\n",
    "    words = re.findall(r'\\b\\w+\\b', s)\n",
    "    if(do_lower):\n",
    "        s = s.lower()\n",
    "        return s.split()\n",
    "    \n",
    "file_path = (r\"/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    file_contents = f.read()\n",
    "    \n",
    "new_count = len(re.findall(r'\\bAlice\\b', file_contents, re.IGNORECASE))  \n",
    "\n",
    "print(\"The word 'alice' occurs = {} times\".format(new_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyrxlSPnnDNz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2(b) - 10 points\n",
    "\n",
    "A spoiler for [2(a)](#part2a): there is a deficiency in how we implemented the `get_words` function. When we are counting words, we probably don’t care whether the word was adjacent to a punctuation mark. For example, the word **hatter** appears in the text 57 times, but if we queried the `count_words` dictionary, we would see it only appeared 24 times. However, it also appeared numerous times adjacent to a punctuation mark, so those instances got counted separately:\n",
    "\n",
    "```\n",
    "word_freq = words_by_frequency(words)\n",
    "for (word, freq) in word_freq:\n",
    "    if 'hatter' in word:\n",
    "        print('{:10} {:3d}'.format(word, freq))\n",
    "\n",
    "hatter      24\n",
    "hatter.     13\n",
    "hatter,     10\n",
    "hatter:      6\n",
    "hatters      1\n",
    "hatter's     1\n",
    "hatter;      1\n",
    "hatter.'     1\n",
    "```\n",
    "\n",
    "Our `get_words` function would be better if it separated punctuation from words. We can accomplish this by using the `re.split` function. Be sure to import `re` to make `re.split()` work. Below is a small example that demonstrates how `str.split` works on a small text and compares it to using `re.split`:\n",
    "\n",
    "```\n",
    "text = '\"Oh no, no,\" said the little Fly, \"to ask me is in vain.\"'\n",
    "text.split()\n",
    "\n",
    "['\"Oh', 'no,', 'no,\"', 'said', 'the', 'little', 'Fly,', '\"to', 'ask', 'me', 'is', 'in', 'vain.\"']\n",
    "\n",
    "re.split(r'(\\W)', text)\n",
    "\n",
    "['', '\"', 'Oh', ' ', 'no', ',', '', ' ', 'no', ',', '', '\"', '', ' ', 'said', ' ', 'the',\n",
    "\n",
    " ' ', 'little', ' ', 'Fly', ',', '', ' ', '', '\"', 'to', ' ', 'ask', ' ', 'me', ' ', 'is',\n",
    "\n",
    " ' ', 'in', ' ', 'vain', '.', '', '\"', '']\n",
    "```\n",
    "\n",
    "Note that this is not exactly what we want, but it is a lot closer. In the resulting list, we find empty strings and spaces, but we have also successfully separated the punctuation from the words.\n",
    "\n",
    "Using the above example as a guide, write and test a function called `tokenize` that takes a string as an input and returns a list of words and punctuation, but not extraneous spaces and empty strings. Like `get_words`, `tokenize` should take an optional argument `do_lower` that determines whether the string should be case normalized before separating the words. You don’t need to modify the `re.split()` line: just remove the empty strings, spaces, and newlines.\n",
    "\n",
    "```\n",
    "tokenize(text, do_lower=True)\n",
    "\n",
    "['\"', 'oh', 'no', ',', 'no', ',', '\"', 'said', 'the', 'little', 'fly', ',', '\"', 'to', 'ask', 'me', 'is', 'in', 'vain', '.', '\"']\n",
    "\n",
    "print(' '.join(tokenize(text, do_lower=True)))\n",
    "\n",
    "\" oh no , no , \" said the little fly , \" to ask me is in vain . \"\n",
    "```\n",
    "\n",
    "**Checking In**\n",
    "\n",
    "Use your `tokenize` function in conjunction with your `count_words` function to list the top 5 most frequent words in **carroll-alice.txt**. You should get this:\n",
    "\n",
    "```\n",
    "'        2871      <-- single quote\n",
    ",        2418      <-- comma\n",
    "the      1642\n",
    ".         988      <-- period\n",
    "and       872\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 2871), (',', 2418), ('the', 1642), ('.', 988), ('and', 872)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the 5 most frequent words in carroll-alice.txt file \n",
    "# Like the previous questions, I started writing a simple function, tokenize, and \n",
    "# then getting the data which is a text file here, and then finding the five \n",
    "# most common words in that text file.\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "f = open('/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt', 'r')\n",
    "data = f.read()\n",
    "\n",
    "def tokenize():\n",
    "    n = int(input(\"Maximum number of results to return: \"))\n",
    "    data1 = data.lower()\n",
    "    split_data = re.split(r'(\\W)',data1)\n",
    "    while (\"\" in split_data):\n",
    "        split_data.remove(\"\")\n",
    "    while (' ' in split_data):\n",
    "        split_data.remove(' ')\n",
    "    while ('\\n' in split_data):\n",
    "        split_data.remove('\\n')       \n",
    "    freq = collections.Counter(split_data)\n",
    "    return freq.most_common(n)\n",
    "\n",
    "tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'\", 2871), (',', 2418), ('the', 1642), ('.', 988), ('and', 872)]\n"
     ]
    }
   ],
   "source": [
    "# Better version of the code here! If you compare these codes, you will get that\n",
    "# enevthough both of them get the same results, but after working on the scratch code\n",
    "# we're gonna get the code below which works much more faster than the previous one and\n",
    "# this is the concept of learning how to write a better and more efficient programming code!\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def tokenize(text, do_lower=True):\n",
    "    if (do_lower):\n",
    "        text = text.lower()\n",
    "    words = re.split(r'(\\W)', text)\n",
    "    words = [w for w in words if w not in (' ', '', '\\n')]\n",
    "    return words\n",
    "\n",
    "\n",
    "with open(r\"/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt\") as f:\n",
    "    file_contents = f.read()\n",
    "    words = tokenize(file_contents, do_lower = True)\n",
    "    counts = count_words(words)  \n",
    "\n",
    "    print (words_by_frequency(counts, n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2(c) - 10 points\n",
    "\n",
    "Write a function called `filter_nonwords` that takes a list of strings as input and returns a new list of strings that excludes anything that isn’t entirely alphabetic. Use the `str.isalpha()` method to determine is a string is comprised of only alphabetic characters.\n",
    "\n",
    "```\n",
    "text = '\"Oh no, no,\" said the little Fly, \"to ask me is in vain.\"'\n",
    "tokens = tokenize(text, do_lower=True)\n",
    "filter_nonwords(tokens)\n",
    "\n",
    "['oh', 'no', 'no', 'said', 'the', 'little', 'fly', 'to', 'ask', 'me', 'is', 'in', 'vain']\n",
    "```\n",
    "\n",
    "Use this function to list the top 5 most frequent words in **carroll-alice.txt**. Confirm that you get the following before moving on:\n",
    "\n",
    "```\n",
    "the      1642\n",
    "and       872\n",
    "to        729\n",
    "a         632\n",
    "it        595\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1642), ('and', 872), ('to', 729), ('a', 632), ('it', 595)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding 5 most frequent words in carroll-alice.txt using filter_nonwords \n",
    "# and str.isalpha method.\n",
    "\n",
    "import collections\n",
    "\n",
    "f = open('/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt', 'r')\n",
    "data = f.read()\n",
    "\n",
    "def filter_nonwords():\n",
    "    n = int(input(\"Maximum number of results to return: \"))\n",
    "    data1 = data.lower()\n",
    "    for char in data1:\n",
    "        if char.isalpha() == False:\n",
    "           data1 = data1.replace(char, \" \")\n",
    "    split_data = re.split(r'(\\W)',data1)\n",
    "    while (\"\" in split_data):\n",
    "        split_data.remove(\"\")\n",
    "    while (' ' in split_data):\n",
    "        split_data.remove(' ')\n",
    "    while ('\\n' in split_data):\n",
    "        split_data.remove('\\n') \n",
    "    freq = collections.Counter(split_data)\n",
    "    return freq.most_common(n)\n",
    "\n",
    "filter_nonwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1642), ('and', 872), ('to', 729), ('a', 632), ('it', 595)]\n"
     ]
    }
   ],
   "source": [
    "# Testing a more proficient code here! If you look at the results from the both codes,\n",
    "# you will get the same results, but again compare the style of coding and comparing the\n",
    "# running time for both of them; you see this one works so fast! I brought both answers here\n",
    "# just to show that I always start with the first coding style in my mind then, if I would \n",
    "# like to find the better and faster way I try to find another version. This is exactly how I \n",
    "# learned coding in python. It takes time, but worth it!\n",
    "\n",
    "def filter_nonwords(lst):\n",
    "    return list(filter(lambda x: x.isalpha(), lst))\n",
    "\n",
    "with open(r\"/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg/carroll-alice.txt\") as f:\n",
    "    file_contents = f.read()\n",
    "    tokens = tokenize(file_contents, do_lower = True)\n",
    "    words = filter_nonwords(tokens)\n",
    "    \n",
    "print (words_by_frequency(words, n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2(d) - 20 points\n",
    "\n",
    "Iterate through all of the files in the **gutenberg** data directory and print out the top 5 words for each. To get a list of all the files in a directory, use the `os.listdir` function:\n",
    "\n",
    "```\n",
    "import os\n",
    "\n",
    "directory = 'data/gutenberg/'\n",
    "files = os.listdir(directory)\n",
    "infile = open(os.path.join(directory, files[0]), 'r', encoding='latin1')\n",
    "```\n",
    "\n",
    "This example also uses the function `os.path.join` that you might want to read about.\n",
    "\n",
    "*Note about encodings:* This `open` function above uses the optional encoding argument to tell Python that the source file is encoded as latin1. Be sure to use this encoding flag to read the files in the **Gutenberg** corpus, as the default (Unicode) won't work!\n",
    "\n",
    "**Token Analysis Questions**\n",
    "\n",
    "Answer the following questions.\n",
    "\n",
    "1. **Most Frequent Word:** Loop through all the files in the **gutenberg** data directory that end in **.txt**. Is **the** always the most common word? If not, what are some other words that show up as the most frequent word (and in which documents)? What do you notice about these words?\n",
    "2. **Impact of Lowercasing:** If you don’t lowercase all the words before you count them, how does this result change, if at all? Discuss what you observe.\n",
    "\n",
    "<font color='red'>Note: If a question (like the one above) asks you to discuss results, that always means both what the results were and what that implies about the world (i.e., your corpus, your method, etc.). A good answer on this sort of question is a paragraph that goes something like \"the result was X, specific interesting examples were X' and X\", this is/isn't surprising because it would imply P or Q, this implies it might be better to do Y / to evaluate Z to learn more\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all the files in gutenverg data directory\n",
    "\n",
    "import os\n",
    "directory = './data/gutenberg/'\n",
    "files = os.listdir(directory)\n",
    "for f in files:\n",
    "   if f.endswith('.txt'):\n",
    "    with open(os.path.join(directory, f), 'r', encoding='latin1') as infile:\n",
    "     s = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3838), ('of', 1940), ('to', 1527), ('and', 1346), ('a', 1163)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the 5 most common words in all gutenberg text files\n",
    "\n",
    "import collections\n",
    "\n",
    "f = open('/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/brown/editorial.txt', 'r')\n",
    "data = f.read()\n",
    "\n",
    "def filter_nonwords():\n",
    "    n = int(input(\"Maximum number of results to return: \"))\n",
    "    data1 = data.lower()\n",
    "    for char in data1:\n",
    "        if char.isalpha() == False:\n",
    "           data1 = data1.replace(char, \" \")\n",
    "    split_data = re.split(r'(\\W)',data1)\n",
    "    while (\"\" in split_data):\n",
    "        split_data.remove(\"\")\n",
    "    while (' ' in split_data):\n",
    "        split_data.remove(' ')\n",
    "    while ('\\n' in split_data):\n",
    "        split_data.remove('\\n') \n",
    "    freq = collections.Counter(split_data)\n",
    "    return freq.most_common(n)\n",
    "\n",
    "filter_nonwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pu7nnpwBnT6O",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<font color='red'>Your answers go here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Most frequent word: If you look at the results which show the most frequent words in each text files we have here, we can find out 'the' is always one of the most common word. This is not surprising because 'the' is the most word in the English-speaking world because it's an essential partof grammar and communication. It would be difficult to speak English without repeatedly using'the'. Therefore, based on the programming code I developed here, I can confirmed this easily!\n",
    "\n",
    "austen-emma.txt: [('to', 5242), ('the', 5204), ('and', 4897), ('of', 4293), ('i', 3192)]\n",
    "\n",
    "austen-persuasion.txt: [('the', 3329), ('to', 2808), ('and', 2801), ('of', 2570), ('a', 1595)] \n",
    "\n",
    "austen-sense.txt: [('to', 4116), ('the', 4105), ('of', 3572), ('and', 3491), ('her', 2551)]\n",
    "\n",
    "blake-poems.txt: [('the', 439), ('and', 348), ('of', 146), ('in', 141), ('i', 130)]\n",
    "\n",
    "bryant-stories.txt: [('the', 3452), ('and', 2099), ('to', 1180), ('a', 1036), ('he', 1021)]\n",
    "\n",
    "burgess-busterbrown.txt:[('he', 678), ('the', 660), ('and', 516), ('to', 436), ('of', 342)]\n",
    "\n",
    "carroll-alice.txt: [('the', 1642), ('and', 872), ('to', 729), ('a', 632), ('it', 595)]\n",
    "\n",
    "chesterton-ball.txt: [('the', 4981), ('and', 2667), ('of', 2555), ('a', 2263), ('to', 1580)]\n",
    "\n",
    "chesterton-brown.txt: [('the', 4670), ('and', 2221), ('a', 2132), ('of', 2093), ('to', 1391)]\n",
    "\n",
    "chesterton-thursday.txt: [('the', 3636), ('a', 1742), ('of', 1725), ('and', 1658), ('he', 1126)]\n",
    "\n",
    "edgeworth-parents.txt: [('the', 7728), ('to', 5220), ('and', 4983), ('of', 3745), ('i', 3674)]\n",
    "\n",
    "melville-moby_dick.txt: [('the', 14431), ('of', 6609), ('and', 6430), ('a', 4736), ('to', 4625)]\n",
    "\n",
    "milton-paradise.txt: [('and', 3395), ('the', 2968), ('to', 2228), ('of', 2050), ('in', 1366)]\n",
    "\n",
    "shakespeare-caesar.txt: [('and', 627), ('the', 579), ('i', 533), ('to', 446), ('you', 391)]\n",
    "\n",
    "shakespeare-hamlet.txt: [('the', 993), ('and', 863), ('to', 685), ('of', 610), ('i', 574)]\n",
    "\n",
    "shakespeare-macbeth.txt: [('the', 650), ('and', 546), ('to', 384), ('i', 348), ('of', 338)]\n",
    "\n",
    "whitman-leaves.txt: [('the', 10113), ('and', 5334), ('of', 4265), ('i', 2933), ('to', 2244)]\n",
    "\n",
    "2) Impact of lowercasing:\n",
    "\n",
    "We checked the lowercase and uppercase from the begining of this small project. Based on the results we found out if we do not use lowe_case as a parameter we're gonna get different results because python recognizes same words like 'The' and 'the', two different words, but we actually know they are the same word with the same meaning. So, because just at the begining of the sentence we have words with uppercase, it is better change all of them to lowercase and then start tokenizing and counting.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blake-poems.txt\n",
      "the        439\n",
      "and        348\n",
      "of         146\n",
      "in         141\n",
      "i          130\n",
      "carroll-alice.txt\n",
      "the        1642\n",
      "and        872\n",
      "to         729\n",
      "a          632\n",
      "it         595\n",
      "shakespeare-caesar.txt\n",
      "and        627\n",
      "the        579\n",
      "i          533\n",
      "to         446\n",
      "you        391\n",
      "whitman-leaves.txt\n",
      "the        10113\n",
      "and        5334\n",
      "of         4265\n",
      "i          2933\n",
      "to         2244\n",
      "milton-paradise.txt\n",
      "and        3395\n",
      "the        2968\n",
      "to         2228\n",
      "of         2050\n",
      "in         1366\n",
      "bible-kjv.txt\n",
      "the        64023\n",
      "and        51696\n",
      "of         34670\n",
      "to         13580\n",
      "that       12912\n",
      "austen-persuasion.txt\n",
      "the        3329\n",
      "to         2808\n",
      "and        2801\n",
      "of         2570\n",
      "a          1595\n",
      "melville-moby_dick.txt\n",
      "the        14431\n",
      "of         6609\n",
      "and        6430\n",
      "a          4736\n",
      "to         4625\n",
      "edgeworth-parents.txt\n",
      "the        7728\n",
      "to         5220\n",
      "and        4983\n",
      "of         3745\n",
      "i          3657\n",
      "chesterton-thursday.txt\n",
      "the        3636\n",
      "a          1742\n",
      "of         1725\n",
      "and        1658\n",
      "he         1126\n",
      "burgess-busterbrown.txt\n",
      "he         678\n",
      "the        660\n",
      "and        516\n",
      "to         436\n",
      "of         342\n",
      "chesterton-ball.txt\n",
      "the        4965\n",
      "and        2667\n",
      "of         2555\n",
      "a          2262\n",
      "to         1580\n",
      "austen-emma.txt\n",
      "to         5239\n",
      "the        5201\n",
      "and        4896\n",
      "of         4291\n",
      "i          3178\n",
      "chesterton-brown.txt\n",
      "the        4670\n",
      "and        2221\n",
      "a          2132\n",
      "of         2093\n",
      "to         1391\n",
      "shakespeare-hamlet.txt\n",
      "the        993\n",
      "and        863\n",
      "to         685\n",
      "of         610\n",
      "i          574\n",
      "austen-sense.txt\n",
      "to         4116\n",
      "the        4105\n",
      "of         3572\n",
      "and        3491\n",
      "her        2551\n",
      "shakespeare-macbeth.txt\n",
      "the        650\n",
      "and        546\n",
      "to         384\n",
      "i          348\n",
      "of         338\n",
      "bryant-stories.txt\n",
      "the        3451\n",
      "and        2098\n",
      "to         1180\n",
      "a          1036\n",
      "he         1017\n"
     ]
    }
   ],
   "source": [
    "# Finding the 5 most common words in all gutenberg text files (fast version!)\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "directory = (r\"/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/gutenberg\")\n",
    "files = os.listdir(directory)\n",
    "for file in files:\n",
    "    if re.search('txt', file):\n",
    "        print(file)\n",
    "        with open(os.path.join(directory, file), 'r', encoding='latin1') as file:\n",
    "            file_contents = file.read()\n",
    "            tokens = tokenize(file_contents)\n",
    "            words = filter_nonwords(tokens)\n",
    "            word_freq = words_by_frequency(words, n=5)\n",
    "            for word, freq in word_freq:\n",
    "                print('{:10} {:3d}'.format(word, freq))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0DCb05znZIS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 3: Sentence segmentation - 30 points\n",
    "\n",
    "Next, you will write a simple sentence segmenter.\n",
    "\n",
    "The **data/brown** directory includes three English-language text files taken from the Brown Corpus:\n",
    "\n",
    "- `editorial.txt`\n",
    "- `fiction.txt`\n",
    "- `lore.txt`\n",
    "\n",
    "These files represent large strings of natural language text, with no line breaks nor other special symbols to annotate where sentence splits occur. In the data set you are working with, sentences can only end with one of 5 characters: period, colon, semi-colon, exclamation point and question mark.\n",
    "\n",
    "However, there is a catch: not every period represents the end of a sentence. Many abbreviations (U.S.A., Dr., Mon., etc., etc.) that can appear in the middle of a sentence, and the period does not indicate the end of the sentence. (If you have a phone that uses autocomplete to type, you may already have had annoying experiences where it automatically capitalized words after these abbreviations!) These texts also have many examples where colon is not the end of the sentence. The other three punctuation marks are all nearly unambiguously the ends of a sentence (yes, even semi-colons).\n",
    "\n",
    "For each of the above files, I have also provided a file in the same directory containing the **character index** (counting from 0 for the first character) of each of the actual locations of the ends of sentences:\n",
    "\n",
    "- `editorial-eos.txt`\n",
    "- `fiction-eos.txt`\n",
    "- `lore-eos.txt`\n",
    "\n",
    "Your job is to write a sentence segmenter, and to output the predicted token number of each sentence boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1aMANvf0IqF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3(a) - 10 points\n",
    "\n",
    "Below is some starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQm_L9HT0LA9",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def my_best_segmenter(token_list): \n",
    "    \"\"\" TODO: Replace this with an improved sentence segmenter. \"\"\"\n",
    "    pass\n",
    "\n",
    "def baseline_segmenter(token_list):\n",
    "    all_sentences = []\n",
    "    this_sentence = []\n",
    "    for token in token_list:\n",
    "        this_sentence.append(token)\n",
    "        if token in ['.', ':', ';', '!', '?']:\n",
    "            all_sentences.append(this_sentence)\n",
    "            this_sentence = []\n",
    "    return all_sentences\n",
    "\n",
    "def write_sentence_boundaries(sentence_list, out):\n",
    "    \"\"\" TODO: Write out the token numbers of the sentence boundaries. \"\"\"\n",
    "    pass\n",
    "\n",
    "\"\"\" TODO: Write out the code to parse a text file. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zQxW6uWox-S",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Checking In**\n",
    "\n",
    "Confirm that your code can open the file **data/brown/editorial.txt** and that your code from the previous part splits it into 63,333 tokens.\n",
    "\n",
    "Note: Do not filter out punctuation, since those tokens will be exactly the ones we want to consider as potential sentence boundaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63333\n"
     ]
    }
   ],
   "source": [
    "# Cheching In, using the previous tokenize code to confirm 63,333 tokens in editorial.txt file\n",
    "\n",
    "f = open('/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/brown/editorial.txt', 'r')\n",
    "data = f.read()\n",
    "\n",
    "def tokenize( ):\n",
    "    data1 = data.lower()\n",
    "    split_data = re.split(r'(\\W)',data1)\n",
    "    while (\"\" in split_data):\n",
    "        split_data.remove(\"\")\n",
    "    while (' ' in split_data):\n",
    "        split_data.remove(' ')\n",
    "    while ('\\n' in split_data):\n",
    "        split_data.remove('\\n') \n",
    "    print(len(split_data))\n",
    "\n",
    "tokenize()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63333\n"
     ]
    }
   ],
   "source": [
    "# Checking in part of the question, (final version!)\n",
    "\n",
    "dir = (r\"/Users/koleinif20/Desktop/NLP/Homework 1 - Tokenization and segmentation/data/brown\")\n",
    "with open(os.path.join(dir, 'editorial.txt'), 'r', encoding='latin1') as f:\n",
    "    tokens = tokenize(f.read(), do_lower = True)\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3(b) - 10 points\n",
    "\n",
    "The starter code contains a function called `baseline_segmenter` that takes a list of tokens as its only argument. It returns a list of tokenized sentences; that is, a list of lists of words, with one list per sentence.\n",
    "\n",
    "```\n",
    "baseline_segmenter(tokenize('I am Sam. Sam I am.')\n",
    "\n",
    "[['I', 'am', 'Sam', '.'], ['Sam', 'I', 'am', '.']]\n",
    "```\n",
    "\n",
    "Remember that every sentence in our data set ends with one of the five tokens \\['.', ':', ';', '!', '?'\\]. Since it’s a baseline approach, `baseline_segmenter` predicts that every instance of one of these characters is the end of a sentence.\n",
    "\n",
    "Fill in the function `write_sentence_boundaries`. This function takes two arguments: a list of lists of strings (like the one returned by `baseline_segmenter`) and a pointer to a stream to write output (an open write-enabled file). You will need to loop through all of the sentences in the document. For each sentence, you will want to write the index of the last word in the sentence to the filepointer. Remember that Python lists are 0-indexed!\n",
    "\n",
    "Confirm that when you run `baseline_segmenter` on the file **data/brown/editorial.txt**, it predicts 3278 sentence boundaries, and that the first five predicted boundaries are at tokens 22, 54, 74, 99, and 131."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 3278 sentence boundaries.\n",
      "Predicted boundary at token 23\n",
      "Predicted boundary at token 32\n",
      "Predicted boundary at token 20\n",
      "Predicted boundary at token 25\n",
      "Predicted boundary at token 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def baseline_segmenter(token_list):\n",
    "    all_sentences = []\n",
    "    this_sentence = []\n",
    "    for token in token_list:\n",
    "        this_sentence.append(token)\n",
    "        if token in ['.', ':', ';', '!', '?']:\n",
    "            all_sentences.append(this_sentence)\n",
    "            this_sentence = []\n",
    "    return all_sentences   \n",
    "\n",
    "def write_sentence_boundaries(sentences, filepointer):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        filepointer.write(f\"{len(sentence)-1}\\n\")\n",
    "        if i < 5:\n",
    "            print(f\"Predicted boundary at token {len(sentence)}\")\n",
    "\n",
    "with open(os.path.join(dir, 'editorial.txt'), 'r', encoding='latin1') as f:\n",
    "    tokens = tokenize(f.read(), do_lower = True)\n",
    "    segmented_tokens = baseline_segmenter(tokens)\n",
    "    print(f\"We have found {len(segmented_tokens)} sentence boundaries.\")\n",
    "    write_sentence_boundaries(segmented_tokens, open(\"predicted_boundaries.txt\", \"w\"))\n",
    "    sentences = baseline_segmenter(tokens)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGAtIVFhoGJn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3(c) - extra credit, 10 points\n",
    "\n",
    "Now it’s time to improve the baseline sentence segmenter. We don’t have any false negatives (since we’re predicting that every instance of the possibly-end-of-sentence punctuation marks is, in fact, the end of a sentence), but we have quite a few false positives.\n",
    "\n",
    "There’s a placeholder for a second segmentation function defined in the starter code. You will fill in that `my_best_segmenter` function to do a (hopefully!) better job identifying sentence boundaries. The specifics of how you do so are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 3364 sentence boundaries.\n",
      "Predicted boundary at token 23\n",
      "Predicted boundary at token 32\n",
      "Predicted boundary at token 20\n",
      "Predicted boundary at token 25\n",
      "Predicted boundary at token 32\n"
     ]
    }
   ],
   "source": [
    "#extra credit\n",
    "import os\n",
    "\n",
    "def my_best_segmenter(token_list):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    end_punctuations = ['.', ':', ';', '!', '?', \")\", \"]\", \"}\",\"''\"]\n",
    "    for token in token_list:\n",
    "        current_sentence.append(token)\n",
    "        if token in end_punctuations:\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "    return sentences\n",
    "\n",
    "def write_sentence_boundaries(sentences, file):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        file.write(f\"{len(sentence) - 1}\\n\")\n",
    "        if i < 5:\n",
    "            print(f\"Predicted boundary at token {len(sentence)}\")\n",
    "\n",
    "\n",
    "with open(os.path.join(dir, 'editorial.txt'), 'r', encoding='latin1') as f:\n",
    "    tokens = tokenize(f.read(), do_lower = True)\n",
    "    segmented_tokens = my_best_segmenter(tokens)\n",
    "    print(f\"We have found {len(segmented_tokens)} sentence boundaries.\")\n",
    "    with open(\"predicted_boundaries.txt\", \"w\") as boundary_file:\n",
    "        write_sentence_boundaries(segmented_tokens, boundary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UurBjd6m8dGE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions\n",
    "\n",
    "1. Describe the performance of your final segmenter, by comparing some of the sentences it correctly tokenized/generated whereas the `baseline_segmenter` did not.\n",
    "2. Describe at least 3 things that your final segmenter does better than the baseline segmenter and discuss them. What cases are you most proud of catching in your segmenter? Include specific examples that are handled well.\n",
    "3. Describe at least 3 places where your segmenter still makes mistakes and discuss them. Include specific examples where your segmenter makes the wrong decision. If you had another week to work on this, what would you add? If you had the rest of the semester to work on it, what would you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pu7nnpwBnT6O",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<font color='red'>Your answers go here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) If you compare my_best_segmenter and baseline_segmenter, you can figure it out;\n",
    "the number of types of punctuations in best segmenter is more than the baseline_one. So, based on the results we can get that some of the sentences it correctly tokenized/generated whereas the baseline_segmenter could not do that.\n",
    "\n",
    "2) first: Handling abbreviations: abbreviations can cause false positive in the baseline segmenter. A final segmenter can have a list of commonly used abbreviations and ignore the end of the sentence punctuation after them. \n",
    "\n",
    "Secondly: Considering context: in some cases the end of sentence punctuation mark may not signal the end of the sentence.\n",
    "\n",
    "Thirdly: handling unsual punctuations: the baseline_segmenter method only takes into account the commonly recognized end of sentence punctuation marks, ignoring any unconventional forms of punctuations.\n",
    "\n",
    "3) First: Addressing Complex Structures: In examples where the text contains complex arrangements such as nested phrases, parentheses or dashes, it can result in false negatives or false positives in the final segmentation.\n",
    "\n",
    "Secondly:\n",
    "Dealing with Numbers: When dealing with numbers that include decimal points or units, false positives may occur. There is a way to tackle this problem; a final segmenter may implement regular expressions to identify numbers and disregard end-of-sentence punctuation that appears after them. \n",
    "\n",
    "Thirdly:\n",
    "Handling Punctuation Errors: At times, the text may contain incorrect or absent punctuation, resulting in false negatives or false positives. To resolve this, the final segmenter can utilize a correction module to repair the punctuation and enhance its performance.\n",
    "\n",
    "If i had another week i would add a module to tackle complex words and to work on minizing the punctuations errors like false positives errors.\n",
    "\n",
    "If i had rest of the semester to work on it, I would train a model to make decision and provide accurate results."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "02. Python 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "3f397720aeab5d6ca451ef8f0ac36045b34cc58f655e162f506e23ec1d195a3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
